{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 10:06:31 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "[Stage 0:>                                                         (0 + 6) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 10:06:41 WARN  TaskSetManager:66 - Stage 0 contains a task of very large size (111 KB). The maximum recommended task size is 100 KB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=======================================>               (148 + 7) / 206]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 10:06:43 WARN  TaskSetManager:66 - Stage 1 contains a task of very large size (111 KB). The maximum recommended task size is 100 KB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from script import DataManager\n",
    "\n",
    "dm = DataManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now have access to several DataFrames - after running the cell above:\n",
    "\n",
    "- `dm.train_df`  \n",
    "    all *train-*.csv* files concatenated into 1 **DataFrame**\n",
    "- `dm.spark_train_df`  \n",
    "    a Spark Dataframe of all training data\n",
    "- `dm.writing_df`  \n",
    "    the *writing.json* file parsed into a **DataFrame**\n",
    "- `dm.directing_df`  \n",
    "    the *directing.json* file parsed into a **DataFrame**\n",
    "- `dm.joined_df`   \n",
    "    result of `writing_df` left-joined with `directing_df` on column *movie*)\n",
    "- `df.validation_df`  \n",
    "    the *validation_hidden.csv* file as a **DataFrame**\n",
    "- `dm.spark_validation_df`  \n",
    "    a Spark Dataframe of all validation data\n",
    "- `dm.test_df`  \n",
    "    the *test_hidden.csv* file as a **DataFrame**\n",
    "- `dm.spark_test_df`  \n",
    "    a Spark Dataframe of all test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE MOVIE DATABASE API\n",
    "\n",
    "\n",
    "### This API is completely independent from IMDb, since it has its own database. With the code below we can find additional data on each movie. We split up the work in three parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting all movie IDs\n",
    "\n",
    "\n",
    "> 1) First we need all the *IMDB_IDs* which we extract over all three datasets\n",
    "\n",
    "##### Making a mapping from IMDB to TMDB\n",
    "\n",
    "> 2) Using these *IMDB_IDs* we can get the *TMDB_ID* that corresponds to this movie using the API. If such a mapping already exists we don't send out the request. The complete mapping is saved in _imdb/data/api/imdb_to_tmdb_ids.json_.\n",
    "\n",
    "\n",
    "\n",
    "#### Getting the actual data for each movie\n",
    "\n",
    "> 3) Using the *TMDB_IDs* we can get extra data, which comes back as a JSON. I have selected the columns we already want from the beginning and loaded in a country information dataset. This dataset maps a country code to the region where it resides. This way we can see where the movie is made. If there is no mapping possible this is being set to `World`. The genres are collected and a DataFrame is created. We eventually write it away to the _parquet_ format.  \n",
    "\n",
    "##### Steps 1, 2 and 3 are made easier implemented using the DataManager.\n",
    "\n",
    "IMPORTANT: All scraped data can then be found in **dm.scraped_data_location**, which is saved as a lightweight *parquet* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 10:06:57 WARN  TaskSetManager:66 - Stage 7 contains a task of very large size (111 KB). The maximum recommended task size is 100 KB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:08<00:00, 1220.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# step 1 and 2 \n",
    "dm.update_imdb_to_tmdb_id_mapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9954/9954 [00:00<00:00, 64688.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# step 3\n",
    "dm.update_scraped_movie_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 10:07:15 WARN  TaskSetManager:66 - Stage 10 contains a task of very large size (111 KB). The maximum recommended task size is 100 KB.\n",
      "7959\n",
      "(7959, 8)\n",
      "955\n",
      "(955, 7)\n",
      "1086\n",
      "(1086, 7)\n"
     ]
    }
   ],
   "source": [
    "print(dm.spark_train_df.count())\n",
    "print(dm.train_df.shape)\n",
    "\n",
    "print(dm.spark_validation_df.count())\n",
    "print(dm.validation_df.shape)\n",
    "\n",
    "print(dm.spark_test_df.count())\n",
    "print(dm.test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Pipeline using PySpark functionality \n",
    "---------------\n",
    "\n",
    "##### With the additional data in place we can train our model(s).\n",
    "\n",
    "> 1) We first prepare the datasets, joining what needs to be joines and changing column datatypes for example.  \n",
    "\n",
    "> 2) We then create the Transformer and Estimator objects available from the __pyspark.ml__ library, and put this together in a Pipeline object.    \n",
    "\n",
    "> 3) With the Pipeline we can fit our model.  \n",
    "\n",
    "> 4) We can then make predictions and save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm.X_train.printSchema()\n",
    "# dm.X_train.withColumn(\"runtimeMinutes\",dm.X_train.runtimeMinutes.cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dm.X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 10:10:13 WARN  TaskSetManager:66 - Stage 49 contains a task of very large size (111 KB). The maximum recommended task size is 100 KB.\n",
      "(7924, 34)\n"
     ]
    }
   ],
   "source": [
    "print((dm.X_train.count(), len(dm.X_train.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 10:09:51 WARN  Utils:66 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 10:09:53 WARN  TaskSetManager:66 - Stage 18 contains a task of very large size (111 KB). The maximum recommended task size is 100 KB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 10:09:55 WARN  TaskSetManager:66 - Stage 20 contains a task of very large size (111 KB). The maximum recommended task size is 100 KB.\n",
      "2022-03-24 10:09:57 WARN  TaskSetManager:66 - Stage 25 contains a task of very large size (111 KB). The maximum recommended task size is 100 KB.\n",
      "2022-03-24 10:09:57 WARN  TaskSetManager:66 - Stage 27 contains a task of very large size (111 KB). The maximum recommended task size is 100 KB.\n",
      "2022-03-24 10:09:58 WARN  TaskSetManager:66 - Stage 29 contains a task of very large size (111 KB). The maximum recommended task size is 100 KB.\n",
      "2022-03-24 10:09:58 WARN  TaskSetManager:66 - Stage 30 contains a task of very large size (111 KB). The maximum recommended task size is 100 KB.\n",
      "2022-03-24 10:09:59 WARN  TaskSetManager:66 - Stage 31 contains a task of very large size (111 KB). The maximum recommended task size is 100 KB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 10:10:01 WARN  TaskSetManager:66 - Stage 33 contains a task of very large size (111 KB). The maximum recommended task size is 100 KB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-24 10:10:02 WARN  TaskSetManager:66 - Stage 35 contains a task of very large size (111 KB). The maximum recommended task size is 100 KB.\n",
      "2022-03-24 10:10:02 WARN  TaskSetManager:66 - Stage 37 contains a task of very large size (111 KB). The maximum recommended task size is 100 KB.\n",
      "2022-03-24 10:10:02 WARN  TaskSetManager:66 - Stage 39 contains a task of very large size (111 KB). The maximum recommended task size is 100 KB.\n",
      "2022-03-24 10:10:02 WARN  TaskSetManager:66 - Stage 41 contains a task of very large size (111 KB). The maximum recommended task size is 100 KB.\n",
      "2022-03-24 10:10:03 WARN  TaskSetManager:66 - Stage 43 contains a task of very large size (111 KB). The maximum recommended task size is 100 KB.\n"
     ]
    }
   ],
   "source": [
    "# this step creates dm.X_train, dm.X_validation and dm.X_test\n",
    "dm.prepare_datasets()\n",
    "\n",
    "rfc_pipeline = dm.setup_ml_pipeline()\n",
    "trained_model = dm.fit_model(rfc_pipeline)\n",
    "\n",
    "dm.make_predictions(\n",
    "    model=trained_model, \n",
    "    unseen_data=dm.X_validation, \n",
    "    original_left=dm.validation_df.copy(), \n",
    "    filename_for_saving=\"validation_TESTRUN.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8196d58475ab219c15789513678ff250087c2b6cddfbae49b408a898ff292dd9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('bd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
